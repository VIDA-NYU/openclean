{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking Violations - Street Name Clustering (Performance comparison)\n",
    "\n",
    "This notebook evaluates run times for clustering large datasets. It compares two options:\n",
    "\n",
    "- create clusters using the stream operator, and\n",
    "- extract distinct value stes using the stream operator and then cluster separately using multiple threads,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'Parking Violations Issued - Fiscal Year 2014' in file ./jt7v-77mi.tsv.gz of size 379.19 MB\n"
     ]
    }
   ],
   "source": [
    "# Download the full 'NYC Parking Violations Issued - Fiscal Year 2014' dataset.\n",
    "# Note that the downloaded full dataset file is about 380 MB in size! Use the\n",
    "# alternative data file with 10,000 rows that is included in the repository if\n",
    "# you do not want to download the full data file.\n",
    "\n",
    "import gzip\n",
    "import humanfriendly\n",
    "import os\n",
    "\n",
    "from openclean.data.source.socrata import Socrata\n",
    "\n",
    "dataset = Socrata().dataset('jt7v-77mi')\n",
    "datafile = './jt7v-77mi.tsv.gz'\n",
    "\n",
    "# Download file only if it does not exist already.\n",
    "if not os.path.isfile(datafile):\n",
    "    with gzip.open(datafile, 'wb') as f:\n",
    "        print('Downloading ...\\n')\n",
    "        dataset.write(f)\n",
    "\n",
    "\n",
    "# As an alternative, you can also use the smaller dataset sample that is\n",
    "# included in the repository.\n",
    "#\n",
    "#datafile = './data/jt7v-77mi.tsv.gz'\n",
    "\n",
    "fsize = humanfriendly.format_size(os.stat(datafile).st_size)\n",
    "print(\"Using '{}' in file {} of size {}\".format(dataset.name, datafile, fsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data stream for the downloaded file.\n",
    "\n",
    "from openclean.pipeline import stream\n",
    "\n",
    "ds = stream(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Clusters vs. Mutiple-Threads\n",
    "\n",
    "Compare run time for using the stream operator to perform the clustering or extracting the list of distinct street names first and then run clustering in parallel (using up to 4 threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from openclean.cluster.key import KeyCollision\n",
    "from openclean.function.value.key.fingerprint import Fingerprint\n",
    "from openclean_geo.address.usstreet import StandardizeUSStreetName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse time 57.0615 sec. (115567 streets)\n"
     ]
    }
   ],
   "source": [
    "# Extract list of distinct street names.\n",
    "\n",
    "start_parse = time.perf_counter()\n",
    "\n",
    "streets = ds.distinct_values('Street')\n",
    "\n",
    "end_parse = time.perf_counter()\n",
    "\n",
    "print('Parse time {:0.4f} sec. ({} streets)'.format(end_parse - start_parse, len(streets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization time (using 1 threads) 3.0328 sec. (115567 streets)\n",
      "Standardization time (using 2 threads) 2.2598 sec. (115567 streets)\n",
      "Standardization time (using 3 threads) 1.6198 sec. (115567 streets)\n",
      "Standardization time (using 4 threads) 1.6093 sec. (115567 streets)\n"
     ]
    }
   ],
   "source": [
    "# Standardize street names in parallel.\n",
    "\n",
    "f = StandardizeUSStreetName(characters='upper', alphanum=True, repeated=False)\n",
    "\n",
    "for threads in range(1,5):\n",
    "    start_std = time.perf_counter()\n",
    "    streets_std = f.apply(streets, threads=threads)\n",
    "    count = len(streets_std)\n",
    "    end_std = time.perf_counter()\n",
    "    exec_time = end_std - start_std\n",
    "    print('Standardization time (using {} threads) {:0.4f} sec. ({} streets)'.format(threads, exec_time, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster time (using 1 threads) 1.6068 sec. (761 clusters)\n",
      "Cluster time (using 2 threads) 1.1699 sec. (761 clusters)\n",
      "Cluster time (using 3 threads) 1.0394 sec. (761 clusters)\n",
      "Cluster time (using 4 threads) 1.0042 sec. (761 clusters)\n"
     ]
    }
   ],
   "source": [
    "# Cluster standardized streen names.\n",
    "\n",
    "for threads in range(1,5):\n",
    "    f = KeyCollision(func=Fingerprint(), threads=threads)\n",
    "    start_clstr = time.perf_counter()\n",
    "    clusters = f.clusters(streets_std)\n",
    "    count = len(clusters)\n",
    "    end_clstr = time.perf_counter()\n",
    "    exec_time = end_clstr - start_clstr\n",
    "    print('Cluster time (using {} threads) {:0.4f} sec. ({} clusters)'.format(threads, exec_time, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime 310.8050 sec. (761 clusters)\n"
     ]
    }
   ],
   "source": [
    "# Run standardization and cluster generation as part of the\n",
    "# stream processing.\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "streets = ds\\\n",
    "    .select('Street')\\\n",
    "    .update('Street', StandardizeUSStreetName(characters='upper', alphanum=True, repeated=False))\n",
    "\n",
    "clusters = streets.cluster(clusterer=KeyCollision(func=Fingerprint()))\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print('Runtime {:0.4f} sec. ({} clusters)'.format(end - start, len(clusters)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
